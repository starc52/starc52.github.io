<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: Georgia, serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-size:32px;
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
        5px 5px 0 0px #fff, /* The second layer */
        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
        10px 10px 0 0px #fff, /* The third layer */
        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
        15px 15px 0 0px #fff, /* The fourth layer */
        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
        20px 20px 0 0px #fff, /* The fifth layer */
        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
        25px 25px 0 0px #fff, /* The fifth layer */
        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }

    .paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
        5px 5px 0 0px #fff, /* The second layer */
        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
        10px 10px 0 0px #fff, /* The third layer */
        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
<head>
    <title>JEAN</title>
    <meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
    <meta property="og:title" content="Creative and Descriptive Paper Title." />
    <meta property="og:description" content="Paper description." />

    <!-- Get from Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src=""></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-75863369-6');
    </script>
</head>

<body>
    <br>
    <center>
        <span style="font-size:36px">JEAN: Joint Expression and Audio-guided <br> NeRF-based Talking-Face Generation</span><br></br>
        <table align=center width=1200px>
            <table align=center width=1200px>
                <tr>
                    <td align=center width=300px>
                        <center>
                            <span style="font-size:24px">Sai Tanmay Reddy Chakkera</a></span>
                        </center>
                    </td>
                    <td align=center width=300px>
                        <center>
                            <span style="font-size:24px">Aggelina Chatziagapi</a></span>
                        </center>
                    </td>
                    <td align=center width=300px>
                        <center>
                            <span style="font-size:24px">Dimitris Samaras</a></span>
                        </center>
                    </td>
                </tr>
            </table><br></br>
            <table align=center width=1200px>
                <tr>
                    <td align=center width=300px>
                        <center>
                            <span style="font-size:24px">Stony Brook University</a></span>
                        </center>
                    </td>
                </tr>
            </table><br></br>
            <table align=center width=1200px>
                <tr>
                    <td align=center width=300px>
                        <center>
                            <span style="font-size:24px"><i>British Machine Vision Conference 2024</i></a></span>
                        </center>
                    </td>
                </tr>
            </table><br></br>
            <table align=center width=250px>
                <tr>
                    <td align=center width=120px>
                        <center>
                            <span style="font-size:24px"><a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_180/paper.pdf">[Paper]</a></span>
                        </center>
                    </td>
<!--                     <td align=center width=120px>
                        <center>
                            <span style="font-size:24px">[Code Release (01/2025)]</a></span><br>
                        </center>
                    </td> -->
                </tr>
            </table><br></br>
        </table>
    </center>

    <center>
        <table align=center width=1024px>
            <tr>
                <td width=260px>
                    <center>
                        <img class="round" style="width:1024px" src="teaser.png"/>
                    </center>
                </td>
            </tr>
        </table>
        <!-- <table align=center width=850px>
            <tr>
                <td>
                    This was a template originally made for <a href="http://richzhang.github.io/colorization/">Colorful Image Colorization</a>. The code can be found in this <a href="https://github.com/richzhang/webpage-template">repository</a>.
                </td>
            </tr>
        </table> -->
    </center>

    <hr>

    <table align=center width=1024px>
        <center><h1>Abstract</h1></center>
        <tr>
            <td>
                <p style="text-align: center">We introduce a novel method for joint expression and audio-guided talking face generation. Recent approaches either struggle to preserve the speaker identity or fail to produce faithful facial expressions. To address these challenges, we propose a NeRF-based network. Since we train our network on monocular videos without any ground truth, it is essential to learn disentangled representations for audio and expression. We first learn audio features in a self-supervised manner, given utterances from multiple subjects. By incorporating a contrastive learning technique, we ensure that the learned audio features are aligned to the lip motion and disentangled from the muscle motion of the rest of the face. We then devise a transformer-based architecture that learns expression features, capturing long-range facial expressions and disentangling them from the speech-specific mouth movements. Through quantitative and qualitative evaluation, we demonstrate that our method can synthesize high-fidelity talking face videos, achieving state-of-the-art facial expression transfer along with lip synchronization to unseen audio.</p>

            </td>
        </tr>
    </table>
    <br>

  <hr>
    <center><h1>Video</h1></center>
     <p align="center">
        <iframe style="width: 100%; min-height: 576px" src="https://www.youtube.com/embed/iTlW1HyxFDQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
    </p>
    <br>
    <hr>
    <table align=center width=1024px>
        <center><h1>BibTeX</h1></center>
         <tr>
            <td align=left width=1024px>
                If you find our work useful, please consider citing our paper:
            <!-- <td><span style="font-size:14pt"><center> -->
                <!-- <a href="./resources/bibtex.txt">[Bibtex]</a>  -->
            <div>
            <pre style="background-color: #f1f1f1; overflow-x: auto; max-width: 1024px;">
                <code>
                        @inproceedings{chakkera2024jean,
                            &emsp;title={JEAN: Joint Expression and Audio-guided NeRF-based Talking Face Generation},
                            &emsp;author={Sai Tanmay Reddy Chakkera and Aggelina Chatziagapi and Dimitris Samaras},
                            &emsp;booktitle={British Machine Vision Conference},
                            &emsp;year={2024}
                        }
                </code>
            </pre>
        </div>
        </td>
            <!-- </center></td> -->
        </tr>
    </table>
    <br>
    <hr>
<!--    <table align=center width=1024>-->
<!--        <tr>-->
<!--            <td width=400px>-->
<!--                <left>-->
<!--                    <center><h1>Citation</h1></center>-->
<!--                    <p style="text-align: left">-->
<!--                        @inproceedings{chakkera2024jean, <br>-->
<!--                            &emsp;title={JEAN: Joint Expression and Audio-guided NeRF-based Talking Face Generation}, <br>-->
<!--                            &emsp;author={Sai Tanmay Reddy Chakkera and Aggelina Chatziagapi and Dimitris Samaras}, <br>-->
<!--                            &emsp;booktitle={British Machine Vision Conference}, <br>-->
<!--                            &emsp;year={2024} <br>-->
<!--                        }<br>-->
<!--                    </p>-->
<!--                </left>-->
<!--            </td>-->
<!--        </tr>-->
<!--    </table>-->
    <br>
    <hr>
    <table align=center width=1024>
        <tr>
            <td width=400px>
                <left>
                    <center><h1>Acknowledgements</h1></center>
                    <p style="text-align: center">This work was supported in part by Amazon Prime Video and a grant from the CDC/NIOSH (U01 OH012476).</p>
                </left>
            </td>
        </tr>
    </table>

<br>
</body>
</html>
