---
title: "JEAN: Joint Expression and Audio-guided NeRF-based Talking Face Generation"
collection: publications
permalink: /publications/2024-07-19-JEAN
excerpt: 'We introduce a novel method for joint expression and audio-guided talking face generation. Recent approaches either struggle to preserve the speaker identity or fail to produce faithful facial expressions. To address these challenges, we propose a NeRF-based network. Since we train our network on monocular videos without any ground truth, it is essential to learn disentangled representations for audio and expression. We first learn audio features in a self-supervised manner, given utterances from multiple subjects. By incorporating a contrastive learning technique, we ensure that the learned audio features are aligned to the lip motion and disentangled from the muscle motion of the rest of the face. We then devise a transformer-based architecture that learns expression features, capturing long-range facial expressions and disentangling them from the speech-specific mouth movements. Through quantitative and qualitative evaluation, we demonstrate that our method can synthesize high-fidelity talking face videos, achieving state-of-the-art facial expression transfer along with lip synchronization to unseen audio.'
date: 2024-07-19
venue: 'British Machine Vision Conference 2024'
paperurl: 'http://academicpages.github.io/files/paper1.pdf'
citation: 'Chakkera, Sai Tanmay Reddy and Chatziagapi, Aggelina  and Samaras, Dimitris (2024). &quot;JEAN: Joint Expression and Audio-guided NeRF-based Talking Face Generation&quot; <i>British Machine Vision Conference 2024</i>'
---

<a href='http://academicpages.github.io/files/paper1.pdf'>Download paper here</a>

We introduce a novel method for joint expression and audio-guided talking face generation. Recent approaches either struggle to preserve the speaker identity or fail to produce faithful facial expressions. To address these challenges, we propose a NeRF-based network. Since we train our network on monocular videos without any ground truth, it is essential to learn disentangled representations for audio and expression. We first learn audio features in a self-supervised manner, given utterances from multiple subjects. By incorporating a contrastive learning technique, we ensure that the learned audio features are aligned to the lip motion and disentangled from the muscle motion of the rest of the face. We then devise a transformer-based architecture that learns expression features, capturing long-range facial expressions and disentangling them from the speech-specific mouth movements. Through quantitative and qualitative evaluation, we demonstrate that our method can synthesize high-fidelity talking face videos, achieving state-of-the-art facial expression transfer along with lip synchronization to unseen audio.

Recommended citation: Chakkera, Sai Tanmay Reddy and Chatziagapi, Aggelina  and Samaras, Dimitris (2024). "JEAN: Joint Expression and Audio-guided NeRF-based Talking Face Generation" <i>British Machine Vision Conference 2024</i>